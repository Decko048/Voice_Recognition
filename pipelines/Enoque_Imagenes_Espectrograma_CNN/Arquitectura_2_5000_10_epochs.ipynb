{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arquitectura\n",
    "\n",
    "https://medium.com/x8-the-ai-community/audio-classification-using-cnn-coding-example-f9cbd272269e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/julian/anaconda3/envs/py3/bin/python3'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"<string>\", line 1, in <module>\r\n",
      "ModuleNotFoundError: No module named 'tensorflow'\r\n"
     ]
    }
   ],
   "source": [
    "! python -c 'import tensorflow as tf; print(tf.__version__)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D,Dropout,Flatten,MaxPooling2D, Input\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import utils\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/spectrograms/spectrograms_5000.hd5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_hdf(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape images add channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "width=df.Spectrogram[0].shape[1]\n",
    "high=df.Spectrogram[0].shape[0]\n",
    "images_number = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 99, 161)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_dimensions =(1,high,width)\n",
    "images_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_shape = (images_number,high,width) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.zeros(x_shape)\n",
    "cont_images = 0\n",
    "for i in df['Spectrogram'].values:\n",
    "    i.reshape(images_dimensions)\n",
    "    d[cont_images,:,:]=i\n",
    "    cont_images =cont_images +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 99, 161)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape ## matriz con las imagenes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño dataSet (5000, 99, 161)\n",
      "\n",
      "\n",
      "Y shape (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "X = d\n",
    "Y = df.word.values\n",
    "print('Tamaño dataSet', X.shape)\n",
    "print('\\n')\n",
    "Y = np.reshape(Y,(np.size(Y,0),1))\n",
    "print('Y shape',Y.shape)\n",
    "groups = df.speaker.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataSet(X,Y,groups,test_size):              \n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=0)\n",
    "        gss.get_n_splits()\n",
    "        returns = []\n",
    "        for train_index, test_index in gss.split(X, Y, groups=groups):\n",
    "            X_train_original, X_test_original = X[train_index], X[test_index]\n",
    "            Y_train__original, Y_test_original = Y[train_index], Y[test_index]\n",
    "            groups_original = groups[train_index]\n",
    "            groups_test_original=groups[test_index]\n",
    "            returns.append(X_train_original)\n",
    "            returns.append(Y_train__original)\n",
    "            returns.append(X_test_original)\n",
    "            returns.append(Y_test_original)\n",
    "            returns.append(groups_original)\n",
    "            returns.append(groups_test_original) \n",
    "              \n",
    "        return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = splitDataSet(X,Y,groups,test_size)\n",
    "X_train_original = data_train[0]\n",
    "Y_train__original=data_train[1]\n",
    "X_test_original=data_train[2]\n",
    "Y_test_original=data_train[3]\n",
    "groups_original=data_train[4]\n",
    "groups_test_original=data_train[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4111, 99, 161)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(889, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_original.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expandir dimensiones de X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_original= np.expand_dims(X_train_original, axis=3)\n",
    "X_test_original= np.expand_dims(X_test_original, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4111, 99, 161, 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(889, 99, 161, 1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_original.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trainOHE = utils.to_categorical(Y_train__original)\n",
    "y_trainOHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Número de clases\n",
    "num_classes = y_trainOHE.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape =(high,width,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 161, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network architecture using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x7f4b6731f320>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "img= Input(shape=input_shape)\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "validity = model(img)\n",
    "\n",
    "Model(img, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 97, 159, 32)       320       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 95, 157, 64)       18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 47, 78, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 47, 78, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 234624)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               30032000  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 30,051,461\n",
      "Trainable params: 30,051,461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 3699 samples, validate on 412 samples\n",
      "Epoch 1/80\n",
      "3699/3699 [==============================] - 76s 21ms/sample - loss: 27.7553 - accuracy: 0.2493 - val_loss: 2.2638 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/80\n",
      "3699/3699 [==============================] - 77s 21ms/sample - loss: 1.5828 - accuracy: 0.2422 - val_loss: 1.6639 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/80\n",
      "3699/3699 [==============================] - 73s 20ms/sample - loss: 1.5991 - accuracy: 0.2260 - val_loss: 1.7788 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/80\n",
      "3699/3699 [==============================] - 78s 21ms/sample - loss: 1.5913 - accuracy: 0.2233 - val_loss: 1.8941 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/80\n",
      "3699/3699 [==============================] - 78s 21ms/sample - loss: 1.5792 - accuracy: 0.2217 - val_loss: 2.2639 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/80\n",
      "3699/3699 [==============================] - 80s 22ms/sample - loss: 1.5660 - accuracy: 0.2411 - val_loss: 2.0296 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/80\n",
      "3699/3699 [==============================] - 81s 22ms/sample - loss: 1.5535 - accuracy: 0.2609 - val_loss: 2.1344 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 1.5449 - accuracy: 0.2733 - val_loss: 2.1641 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/80\n",
      "3699/3699 [==============================] - 70s 19ms/sample - loss: 1.5442 - accuracy: 0.2806 - val_loss: 2.1207 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/80\n",
      "3699/3699 [==============================] - 70s 19ms/sample - loss: 1.5327 - accuracy: 0.2755 - val_loss: 2.1428 - val_accuracy: 0.0170\n",
      "Epoch 11/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 1.5282 - accuracy: 0.2890 - val_loss: 1.8080 - val_accuracy: 0.0583\n",
      "Epoch 12/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 1.5176 - accuracy: 0.3085 - val_loss: 2.1792 - val_accuracy: 0.0194\n",
      "Epoch 13/80\n",
      "3699/3699 [==============================] - 74s 20ms/sample - loss: 1.5171 - accuracy: 0.2979 - val_loss: 2.1592 - val_accuracy: 0.0316\n",
      "Epoch 14/80\n",
      "3699/3699 [==============================] - 77s 21ms/sample - loss: 1.5207 - accuracy: 0.2925 - val_loss: 1.8001 - val_accuracy: 0.0558\n",
      "Epoch 15/80\n",
      "3699/3699 [==============================] - 87s 24ms/sample - loss: 1.5023 - accuracy: 0.3198 - val_loss: 1.7690 - val_accuracy: 0.1262\n",
      "Epoch 16/80\n",
      "3699/3699 [==============================] - 82s 22ms/sample - loss: 1.4974 - accuracy: 0.3239 - val_loss: 2.0685 - val_accuracy: 0.0583\n",
      "Epoch 17/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 1.4960 - accuracy: 0.3166 - val_loss: 2.0303 - val_accuracy: 0.0728\n",
      "Epoch 18/80\n",
      "3699/3699 [==============================] - 82s 22ms/sample - loss: 1.4891 - accuracy: 0.3168 - val_loss: 1.8069 - val_accuracy: 0.1165\n",
      "Epoch 19/80\n",
      "3699/3699 [==============================] - 72s 19ms/sample - loss: 1.4926 - accuracy: 0.3131 - val_loss: 1.7402 - val_accuracy: 0.1335\n",
      "Epoch 20/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 1.4840 - accuracy: 0.3247 - val_loss: 1.8492 - val_accuracy: 0.1092\n",
      "Epoch 21/80\n",
      "3699/3699 [==============================] - 72s 20ms/sample - loss: 1.4875 - accuracy: 0.3136 - val_loss: 1.6406 - val_accuracy: 0.1311\n",
      "Epoch 22/80\n",
      "3699/3699 [==============================] - 77s 21ms/sample - loss: 1.4838 - accuracy: 0.3301 - val_loss: 1.5902 - val_accuracy: 0.1481\n",
      "Epoch 23/80\n",
      "3699/3699 [==============================] - 70s 19ms/sample - loss: 1.4776 - accuracy: 0.3233 - val_loss: 1.6815 - val_accuracy: 0.1820\n",
      "Epoch 24/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 1.4409 - accuracy: 0.3577 - val_loss: 1.6119 - val_accuracy: 0.3471\n",
      "Epoch 25/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 1.4503 - accuracy: 0.3544 - val_loss: 1.9082 - val_accuracy: 0.0874\n",
      "Epoch 26/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 1.3892 - accuracy: 0.3874 - val_loss: 1.5463 - val_accuracy: 0.1626\n",
      "Epoch 27/80\n",
      "3699/3699 [==============================] - 70s 19ms/sample - loss: 1.2173 - accuracy: 0.4874 - val_loss: 1.4146 - val_accuracy: 0.4126\n",
      "Epoch 28/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.9383 - accuracy: 0.6164 - val_loss: 1.6310 - val_accuracy: 0.2670\n",
      "Epoch 29/80\n",
      "3699/3699 [==============================] - 70s 19ms/sample - loss: 0.8817 - accuracy: 0.6453 - val_loss: 1.5384 - val_accuracy: 0.3568\n",
      "Epoch 30/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.7302 - accuracy: 0.7053 - val_loss: 1.7891 - val_accuracy: 0.3714\n",
      "Epoch 31/80\n",
      "3699/3699 [==============================] - 70s 19ms/sample - loss: 0.6795 - accuracy: 0.7367 - val_loss: 1.4370 - val_accuracy: 0.3932\n",
      "Epoch 32/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.6223 - accuracy: 0.7567 - val_loss: 1.7565 - val_accuracy: 0.3738\n",
      "Epoch 33/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.5858 - accuracy: 0.7762 - val_loss: 1.6572 - val_accuracy: 0.3835\n",
      "Epoch 34/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.5615 - accuracy: 0.7881 - val_loss: 1.8315 - val_accuracy: 0.3010\n",
      "Epoch 35/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.5225 - accuracy: 0.7972 - val_loss: 1.8074 - val_accuracy: 0.4830\n",
      "Epoch 36/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.4900 - accuracy: 0.8140 - val_loss: 1.6232 - val_accuracy: 0.4733\n",
      "Epoch 37/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.4520 - accuracy: 0.8278 - val_loss: 1.6568 - val_accuracy: 0.4854\n",
      "Epoch 38/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.4452 - accuracy: 0.8245 - val_loss: 1.6295 - val_accuracy: 0.5218\n",
      "Epoch 39/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.4112 - accuracy: 0.8508 - val_loss: 1.6519 - val_accuracy: 0.4684\n",
      "Epoch 40/80\n",
      "3699/3699 [==============================] - 70s 19ms/sample - loss: 0.4071 - accuracy: 0.8489 - val_loss: 2.0130 - val_accuracy: 0.4223\n",
      "Epoch 41/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.4008 - accuracy: 0.8510 - val_loss: 1.9764 - val_accuracy: 0.4126\n",
      "Epoch 42/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.3966 - accuracy: 0.8516 - val_loss: 1.8467 - val_accuracy: 0.4442\n",
      "Epoch 43/80\n",
      "3699/3699 [==============================] - 70s 19ms/sample - loss: 0.3555 - accuracy: 0.8643 - val_loss: 1.4918 - val_accuracy: 0.5073\n",
      "Epoch 44/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.3385 - accuracy: 0.8756 - val_loss: 1.7868 - val_accuracy: 0.4515\n",
      "Epoch 45/80\n",
      "3699/3699 [==============================] - 70s 19ms/sample - loss: 0.3188 - accuracy: 0.8838 - val_loss: 2.2236 - val_accuracy: 0.4733\n",
      "Epoch 46/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.3498 - accuracy: 0.8683 - val_loss: 1.9564 - val_accuracy: 0.4345\n",
      "Epoch 47/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.3008 - accuracy: 0.8848 - val_loss: 1.8570 - val_accuracy: 0.5049\n",
      "Epoch 48/80\n",
      "3699/3699 [==============================] - 70s 19ms/sample - loss: 0.3117 - accuracy: 0.8851 - val_loss: 2.0773 - val_accuracy: 0.4806\n",
      "Epoch 49/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.2885 - accuracy: 0.8932 - val_loss: 2.1497 - val_accuracy: 0.4854\n",
      "Epoch 50/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.3045 - accuracy: 0.8921 - val_loss: 2.1029 - val_accuracy: 0.4757\n",
      "Epoch 51/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.2745 - accuracy: 0.9019 - val_loss: 1.6367 - val_accuracy: 0.5752\n",
      "Epoch 52/80\n",
      "3699/3699 [==============================] - 70s 19ms/sample - loss: 0.2784 - accuracy: 0.8946 - val_loss: 2.0127 - val_accuracy: 0.4539\n",
      "Epoch 53/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.2780 - accuracy: 0.8954 - val_loss: 2.1625 - val_accuracy: 0.4612\n",
      "Epoch 54/80\n",
      "3699/3699 [==============================] - 70s 19ms/sample - loss: 0.2597 - accuracy: 0.9019 - val_loss: 1.8987 - val_accuracy: 0.4830\n",
      "Epoch 55/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3699/3699 [==============================] - 73s 20ms/sample - loss: 0.2697 - accuracy: 0.9008 - val_loss: 2.1086 - val_accuracy: 0.4927\n",
      "Epoch 56/80\n",
      "3699/3699 [==============================] - 80s 22ms/sample - loss: 0.2667 - accuracy: 0.9046 - val_loss: 2.2013 - val_accuracy: 0.4272\n",
      "Epoch 57/80\n",
      "3699/3699 [==============================] - 75s 20ms/sample - loss: 0.2710 - accuracy: 0.9005 - val_loss: 2.3666 - val_accuracy: 0.5291\n",
      "Epoch 58/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.2498 - accuracy: 0.9065 - val_loss: 2.4150 - val_accuracy: 0.4854\n",
      "Epoch 59/80\n",
      "3699/3699 [==============================] - 70s 19ms/sample - loss: 0.2265 - accuracy: 0.9208 - val_loss: 2.8836 - val_accuracy: 0.4272\n",
      "Epoch 60/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.2696 - accuracy: 0.9043 - val_loss: 1.9202 - val_accuracy: 0.5049\n",
      "Epoch 61/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.2471 - accuracy: 0.9111 - val_loss: 2.1987 - val_accuracy: 0.4587\n",
      "Epoch 62/80\n",
      "3699/3699 [==============================] - 71s 19ms/sample - loss: 0.2472 - accuracy: 0.9102 - val_loss: 2.2335 - val_accuracy: 0.4806\n",
      "Epoch 63/80\n",
      "1120/3699 [========>.....................] - ETA: 47s - loss: 0.2747 - accuracy: 0.8991"
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "history = model.fit(X_train_original, y_trainOHE, epochs=80, batch_size=80, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,X_test_original):\n",
    "    Yest = model.predict(X_test_original)\n",
    "    Yest = np.argmax(Yest, axis=1)\n",
    "    Yest = Yest.reshape((Yest.shape[0],1))\n",
    "    return Yest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consfusion_matrix(Y_real,preds,nb_classes):\n",
    "    Accuracy = np.mean(preds == Y_real)\n",
    "    print('Accuracy = ', Accuracy*100, '% ')\n",
    "    objects = ('yes','no','right','five','nine')\n",
    "    cm = confusion_matrix(Y_real, preds)\n",
    "    cm = cm/np.sum(cm,axis=0)\n",
    "    cmap = plt.cm.gray\n",
    "    tick_marks = np.arange(nb_classes)\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            text = ax.text(j, i, np.around(cm[i, j],decimals=2),\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "    plt.title('Normalized confusion matrix')\n",
    "    fig.colorbar(im)\n",
    "    plt.xticks(tick_marks, objects, rotation=45)\n",
    "    plt.yticks(tick_marks, objects);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yest  = predict(model,X_test_original)\n",
    "Yest =Yest.flatten()\n",
    "Yest =np.array(Yest)\n",
    "Y_test_original= Y_test_original.flatten()\n",
    "Y_test_original =list(Y_test_original)\n",
    "consfusion_matrix(Y_test_original,Yest,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_Save = '/home/josearangos/Documentos/Projects/Voice_Recognition/pipelines/Enoque_Imagenes_Espectrograma_CNN/models/mode_2500_trained_samples_10_epochs_arquitectura2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(path_model_Save)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julian(python36)",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
