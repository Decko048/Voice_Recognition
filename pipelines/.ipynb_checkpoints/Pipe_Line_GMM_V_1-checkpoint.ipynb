{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import  SVC\n",
    "import math\n",
    "import scipy as sc\n",
    "from scipy import stats\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score,recall_score, f1_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,roc_curve,auc\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHDATA = '../data/audiosData.csv'\n",
    "df = pd.read_csv(PATHDATA, header = None)\n",
    "data = df.values #Convertimos en un  numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:,0:-2]\n",
    "Y = data[:,-1]\n",
    "print('Tamaño dataSet', X.shape)\n",
    "print('\\n')\n",
    "Y=  np.reshape(Y,(np.size(Y,0),1))\n",
    "groups = data[:,-2:-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Número de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=len(np.unique(Y))\n",
    "print('Número de clases:', n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Número de hablantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_groups = len(np.unique(groups))\n",
    "print('Número de hablantes diferentes', n_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir el dataset (train/test) 0.8/0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "gss.get_n_splits()\n",
    "for train_index, test_index in gss.split(X, Y, groups=groups):\n",
    "    X_train_original, X_test__original = X[train_index], X[test_index]\n",
    "    #print(X_train, X_test)\n",
    "    Y_train__original, Y_test__original = Y[train_index], Y[test_index]\n",
    "    #print(y_train, y_test)\n",
    "    groups_original = groups[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train__original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model, parameters, folds, train_size, X,Y,groups_original):\n",
    "  acc_scorer = make_scorer(accuracy_score)\n",
    "  recalls = make_scorer(recall_score,average='micro')##buscar por que micro\n",
    "  precision = make_scorer(precision_score,average='micro')\n",
    "  f1 = make_scorer(f1_score,average='micro')\n",
    "  scores =  {'recalls':recalls,'precision':precision,'f1':f1,'Accuracy': make_scorer(accuracy_score)}\n",
    "  gss = GroupShuffleSplit(n_splits=folds, train_size=train_size, random_state=0)\n",
    "  model = GridSearchCV(model,parameters,scores,-1,refit='Accuracy',return_train_score=True, cv=gss.split(X, Y, groups=groups_original))\n",
    "  model.fit(X,Y)\n",
    "  return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GaussianMixture (GMM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 10\n",
    "TRAIN_SIZE = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mejor modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos X_train_original y Y_train__original para seleccionar la mejor combinación de parametros y posteriormente validar dichos resultados con el conjunto X_test__original y Y_test__original.\n",
    "\n",
    "<b>Nota:</b> el conjunto X_train_original se divide en dos conjuntos train y dev a una proporción de 80 a 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters ={'n_components' : [1,2,3,4,5,6,7,8,9,10,13,15,17,19,21],'covariance_type': ['tied','full','spherical','diag']}\n",
    "GMM =GaussianMixture()#Configurar el modelo\n",
    "model_trained_gmm = build_model(GMM, parameters, FOLDS, TRAIN_SIZE, X_train_original,Y_train__original,groups_original)\n",
    "best_params_gmm = model_trained_gmm.cv_results_['params'][model_trained_gmm.best_index_]\n",
    "print(best_params_gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_acurracy = ['mean_train_Accuracy','mean_test_Accuracy','std_train_Accuracy','std_test_Accuracy']\n",
    "best_acurracy_gmm = []\n",
    "for p in params_acurracy:\n",
    "    parameter_acurracy =model_trained_gmm.cv_results_[p][model_trained_gmm.best_index_]\n",
    "    best_acurracy_gmm.append({p:parameter_acurracy})\n",
    "print(\"Resultados mejor modelo: \", best_params_gmm)\n",
    "best_acurracy_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evalución mejor modelo con el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Metrics(model,X_test,Y_test):\n",
    "\n",
    "  y_predicted = model.predict(X_test)\n",
    "  print('Accuracy: ', accuracy_score(Y_test, y_predicted), '\\n')\n",
    "  report = classification_report(Y_test, y_predicted)\n",
    "  print(\"\\nclassification report :\\n\",report )\n",
    " \n",
    "  # Matriz de confusión\n",
    "  cm = confusion_matrix(Y_test, y_predicted)\n",
    "  # Normalise\n",
    "  cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "  fig, ax = plt.subplots(figsize=(10,10))\n",
    "  sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=np.unique(Y_test), yticklabels=np.unique(Y_test))\n",
    "  plt.ylabel('Actual')\n",
    "  plt.xlabel('Predicted')\n",
    "  plt.show(block=False)\n",
    "  \n",
    "  #sns.heatmap(cm,annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n",
    "  #plt.title(\"Matriz de confusión\",fontsize=20)\n",
    "   \n",
    "    \n",
    "  return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Nota:</b> Usamos los conjuntos X_test__original y Y_test__original para evaluar el comportamiento del mejor modelo con datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_best_estimator = model_trained_gmm.best_estimator_\n",
    "metrics = Metrics(gmm_best_estimator,X_test__original,Y_test__original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Curva de aprendizaje con el mejor modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Nota:</b> Esta curva se realiza con todo el conjunto de datos es decir X y Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curve(model,best_parameters,folds, X,Y,groups=groups, suptitle='', title='', xlabel='Training Set Size', ylabel='Acurracy'):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    suptitle : str\n",
    "        Chart suptitle\n",
    "    title: str\n",
    "        Chart title\n",
    "    xlabel: str\n",
    "        Label for the X axis\n",
    "    ylabel: str\n",
    "        Label for the y axis\n",
    "    Returns\n",
    "    -------\n",
    "    Plot of learning curves\n",
    "    \"\"\"\n",
    "    \n",
    "    # create lists to store train and validation scores\n",
    "    train_score = []\n",
    "    val_score = []\n",
    "    std_train= []\n",
    "    std_val=[]\n",
    "\n",
    "    # create ten incremental training set sizes\n",
    "    training_set_sizes = np.linspace(.1,.9,5).tolist()\n",
    "    # for each one of those training set sizes\n",
    "    \n",
    "    for i in training_set_sizes:  \n",
    "        model_trained = build_model(model, best_parameters, folds, i, X,Y,groups)                \n",
    "        EfficiencyVal= model_trained.cv_results_['mean_test_Accuracy'][model_trained.best_index_]\n",
    "        EfficiencyTrain=model_trained.cv_results_['mean_train_Accuracy'][model_trained.best_index_]\n",
    "        stdTrain=model_trained.cv_results_['std_train_Accuracy'][model_trained.best_index_]\n",
    "        stdVal=model_trained.cv_results_['std_test_Accuracy'][model_trained.best_index_]\n",
    "\n",
    "        # store the scores in their respective lists\n",
    "        train_score.append(EfficiencyTrain)\n",
    "        val_score.append(EfficiencyVal)\n",
    "        std_train.append(stdTrain)\n",
    "        std_val.append(stdVal)\n",
    "\n",
    "    train_score =np.array(train_score)\n",
    "    val_score =np.array(val_score)\n",
    "    std_train =np.array(std_train)\n",
    "    std_val =np.array(std_val)\n",
    "\n",
    "\n",
    "    # plot learning curves\n",
    "    fig, ax = plt.subplots(figsize=(14, 9))\n",
    "    ax.plot(training_set_sizes, train_score, c='gold')\n",
    "    ax.plot(training_set_sizes, val_score, c='steelblue')\n",
    "    \n",
    "    ax.fill_between(training_set_sizes,train_score+std_train,train_score-std_train,facecolor='gold',alpha=0.5)\n",
    "    ax.fill_between(training_set_sizes,val_score+std_val,val_score-std_val,facecolor='steelblue',alpha=0.5)\n",
    "\n",
    "    # format the chart to make it look nice\n",
    "    fig.suptitle(suptitle, fontweight='bold', fontsize='20')\n",
    "    ax.set_title(title, size=20)\n",
    "    ax.set_xlabel(xlabel, size=16)\n",
    "    ax.set_ylabel(ylabel, size=16)\n",
    "    ax.legend(['Train set', 'Test set'], fontsize=16)\n",
    "    ax.tick_params(axis='both', labelsize=12)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    def percentages(x, pos):\n",
    "        \"\"\"The two args are the value and tick position\"\"\"\n",
    "        if x < 1:\n",
    "            return '{:1.0f}'.format(x*100)\n",
    "        return '{:1.0f}%'.format(x*100)\n",
    "\n",
    "    def numbers(x, pos):\n",
    "        \"\"\"The two args are the value and tick position\"\"\"\n",
    "        if x >= 1000:\n",
    "            return '{:1,.0f}'.format(x)\n",
    "        return '{:1.0f}'.format(x)\n",
    "    data = {'Train_Size':training_set_sizes, 'mean_train_Accuracy':train_score,'mean_test_Accuracy':val_score,'std_train_Accuracy':std_train,'std_test_Accuracy':std_val}\n",
    "    df_split_params = pd.DataFrame(data)\n",
    "    return df_split_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_GMM = {\"covariance_type\": [\"diag\"], \"n_components\": [4]}\n",
    "df_split_params =learning_curve(model=GMM,best_parameters=best_params_GMM,folds=10, X=X,Y=Y,groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
